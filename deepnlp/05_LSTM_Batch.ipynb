{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efea40e0558>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import pickle\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = json.load(open('../../dataset/NER_16000_dev.json'))\n",
    "\n",
    "training_data=[]\n",
    "\n",
    "for sent in train:\n",
    "    word=[]\n",
    "    tag=[]\n",
    "    for w,p,t in sent:\n",
    "        word.append(w)\n",
    "        tag.append(t)\n",
    "    training_data.append((word,tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w: to_ix[w], seq))\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = \"<PAD>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시퀀스 길이 분포 파악 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Length = [len(t) for t,l in training_data]\n",
    "distribution = Counter(Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket_config = [(5,5),(10,10),(20,20),(30,30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 버킷에 나눠 담으면서 동시에 <패딩까지> 나중에는 동적으로 패딩하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket = [[],[],[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tr,label in training_data:\n",
    "    length = len(tr)\n",
    "    \n",
    "    for i in range(len(bucket_config)):\n",
    "        if bucket_config[i][0] >= length:\n",
    "            \n",
    "            while len(tr) < bucket_config[i][0]:\n",
    "                tr.append(PAD)\n",
    "                label.append(\"O\")\n",
    "            bucket[i].append((tr,label))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893\n",
      "820\n",
      "717\n",
      "294\n"
     ]
    }
   ],
   "source": [
    "for b in bucket:\n",
    "    print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['네', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['O', 'O', 'O', 'O', 'O']),\n",
       " (['네', '!', '<PAD>', '<PAD>', '<PAD>'], ['O', 'O', 'O', 'O', 'O']),\n",
       " (['1002', '434', '286953', '우리은행', '박종화'], ['O', 'O', 'O', 'B-ORG', 'B-PER']),\n",
       " (['네', 'ㅡ', '<PAD>', '<PAD>', '<PAD>'], ['O', 'O', 'O', 'O', 'O']),\n",
       " (['네', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['O', 'O', 'O', 'O', 'O']),\n",
       " (['네', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['O', 'O', 'O', 'O', 'O']),\n",
       " (['알', '겟', '소', '<PAD>', '<PAD>'], ['O', 'O', 'O', 'O', 'O']),\n",
       " (['회원', '가입', '은', '했', '고'], ['O', 'O', 'O', 'O', 'O']),\n",
       " (['PN', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['O', 'O', 'O', 'O', 'O']),\n",
       " (['안녕', '하', '세요', '고객', '님'], ['O', 'O', 'O', 'B-PER', 'O'])]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2index, tag2index 딕 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NER_LIST = ['B-PER','I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG','B-DATE', 'I-DATE','B-TIME','I-TIME','B-MISC','I-MISC','O']\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "ix_to_word = {v:k for k,v in word_to_ix.items()}\n",
    "\n",
    "tag_to_ix={}\n",
    "i=0\n",
    "for tag in NER_LIST:           \n",
    "    tag_to_ix[tag] = i\n",
    "    i+=1\n",
    "\n",
    "ix_to_tag = {v:k for k,v in tag_to_ix.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 가장 쉬운 길이 10개짜리로 고정해 놓고 배치<br>\n",
    "로스 계산 시에도 패딩까지 계산한다... (나중에 실제 길이 알려줘서 그것만 loss 계산하는 법 고민)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#bucket_id = random.choice(range(len(bucket_config)))\n",
    "bucket_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x=[]\n",
    "train_y=[]\n",
    "for tr,label in bucket[bucket_id]:\n",
    "    temp = prepare_sequence(tr, word_to_ix)\n",
    "    temp = temp.view(1,-1)\n",
    "    train_x.append(temp)\n",
    "    \n",
    "    temp2 = prepare_sequence(label,tag_to_ix)\n",
    "    temp2 = temp2.view(1,-1)\n",
    "    train_y.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = bucket_config[bucket_id][0]\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "BATCH_SIZE=10\n",
    "nb_epochs = 10\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = torch.cat(train_x[:BATCH_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,hidden_size, num_layers, num_classes,vocab_size,embedding_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial states \n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) \n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        embeds = self.word_embeddings(x)\n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.lstm(embeds, (h0, c0))  \n",
    "        \n",
    "        tag_space = self.fc(out.contiguous().view(x.size(0)*x.size(1),-1)) # input_length,batch_size,hidden_dim -> input_length*batch_size,hidden_dim\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        \n",
    "        return tag_scores\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RNN(HIDDEN_DIM, num_layers,len(tag_to_ix),len(word_to_ix),EMBEDDING_DIM)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "losses=[]\n",
    "for epoch in range(nb_epochs):\n",
    "    \n",
    "    for offset in range((len(train_x)//BATCH_SIZE)):\n",
    "        inputs = torch.cat(train_x[BATCH_SIZE*offset:BATCH_SIZE*(offset+1)])\n",
    "        targets = torch.cat(train_y[BATCH_SIZE*offset:BATCH_SIZE*(offset+1)])\n",
    "        \n",
    "        tag_scores = model(inputs)\n",
    "        loss = loss_function(tag_scores, targets.view(BATCH_SIZE*INPUT_SIZE))\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.6698\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.3381\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(losses[0],losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
