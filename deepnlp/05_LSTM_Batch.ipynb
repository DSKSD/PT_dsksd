{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f007412e630>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = json.load(open('../../dataset/NER_16000_train.json'))\n",
    "\n",
    "training_data=[]\n",
    "\n",
    "for sent in train:\n",
    "    word=[]\n",
    "    tag=[]\n",
    "    for w,p,t in sent:\n",
    "        word.append(w)\n",
    "        tag.append(t)\n",
    "    training_data.append((word,tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = [t for t in training_data if len(t[0])!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11196"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w: to_ix[w], seq))\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = \"<PAD>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시퀀스 길이 분포 파악 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Length = [len(t) for t,l in training_data]\n",
    "distribution = Counter(Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket_config = [(5,5),(10,10),(20,20),(30,30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 버킷에 나눠 담으면서 동시에 <패딩까지> 나중에는 동적으로 패딩하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket = [[],[],[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tr,label in training_data:\n",
    "    length = len(tr)\n",
    "    \n",
    "    for i in range(len(bucket_config)):\n",
    "        if bucket_config[i][0] >= length:\n",
    "            \n",
    "            while len(tr) < bucket_config[i][0]:\n",
    "                tr.append(PAD)\n",
    "                label.append(\"O\")\n",
    "            bucket[i].append((tr,label))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3184\n",
      "2824\n",
      "2568\n",
      "998\n"
     ]
    }
   ],
   "source": [
    "for b in bucket:\n",
    "    print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(bucket,bucket_id,batch_size):\n",
    "    random.shuffle(bucket[bucket_id])\n",
    "    train_x=[]\n",
    "    train_y=[]\n",
    "    lengths=[]\n",
    "    for tr,label in bucket[bucket_id][:batch_size]:\n",
    "        temp = prepare_sequence(tr, word_to_ix)\n",
    "        temp = temp.view(1,-1)\n",
    "        train_x.append(temp)\n",
    "        \n",
    "        temp2 = prepare_sequence(label,tag_to_ix)\n",
    "        temp2 = temp2.view(1,-1)\n",
    "        train_y.append(temp2)\n",
    "        \n",
    "        length = [t for t in tr if t !='<PAD>']\n",
    "        lengths.append(len(length))\n",
    "    inputs = torch.cat(train_x)\n",
    "    targets = torch.cat(train_y)\n",
    "    \n",
    "     ### PAD 제외하고 로스 계산 ###\n",
    "    t_out=[]\n",
    "    for i in range(len(lengths)):\n",
    "        t_out.append(targets[i][:lengths[i]])\n",
    "    \n",
    "    r_targets = torch.cat(t_out)\n",
    "    \n",
    "    del train_x\n",
    "    del train_y\n",
    "    del t_out\n",
    "\n",
    "    \n",
    "    return inputs,r_targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2index, tag2index 딕 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NER_LIST = ['B-PER','I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG','B-DATE', 'I-DATE','B-TIME','I-TIME','B-MISC','I-MISC','O']\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "ix_to_word = {v:k for k,v in word_to_ix.items()}\n",
    "\n",
    "tag_to_ix={}\n",
    "i=0\n",
    "for tag in NER_LIST:           \n",
    "    tag_to_ix[tag] = i\n",
    "    i+=1\n",
    "\n",
    "ix_to_tag = {v:k for k,v in tag_to_ix.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 가장 쉬운 길이 10개짜리로 고정해 놓고 배치<br>\n",
    "로스 계산 시에도 패딩까지 계산한다... (나중에 실제 길이 알려줘서 그것만 loss 계산하는 법 고민)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#bucket_id = random.choice(range(len(bucket_config)))\n",
    "bucket_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x=[]\n",
    "train_y=[]\n",
    "for tr,label in bucket[bucket_id]:\n",
    "    temp = prepare_sequence(tr, word_to_ix)\n",
    "    temp = temp.view(1,-1)\n",
    "    train_x.append(temp)\n",
    "    \n",
    "    temp2 = prepare_sequence(label,tag_to_ix)\n",
    "    temp2 = temp2.view(1,-1)\n",
    "    train_y.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = bucket_config[bucket_id][0]\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "BATCH_SIZE= 64\n",
    "NUM_LAYERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,hidden_size, num_layers, num_classes,vocab_size,embedding_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x,length):\n",
    "        # Set initial states \n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) \n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        embeds = self.word_embeddings(x)\n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.lstm(embeds, (h0, c0))  \n",
    "        \n",
    "        ### PAD 제외하고 로스 계산 ###\n",
    "        t_out=[]\n",
    "        for i in range(len(length)):\n",
    "            t_out.append(out[i][:length[i]])\n",
    "            \n",
    "        outwithoutpad = torch.cat(t_out)\n",
    "        del t_out\n",
    "        \n",
    "        tag_space = self.fc(outwithoutpad) # input_length,batch_size,hidden_dim -> input_length*batch_size,hidden_dim\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        \n",
    "        \n",
    "        return tag_scores\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNN(HIDDEN_DIM, NUM_LAYERS,len(tag_to_ix),len(word_to_ix),EMBEDDING_DIM)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y,l=getBatch(bucket,1,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o = model(x,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.6222 -2.5880 -2.5342  ...  -2.5593 -2.4500 -2.5203\n",
       "-2.6299 -2.5856 -2.5293  ...  -2.5702 -2.4533 -2.5006\n",
       "-2.6340 -2.5838 -2.5256  ...  -2.5759 -2.4569 -2.4942\n",
       "          ...             ⋱             ...          \n",
       "-2.6481 -2.5844 -2.5272  ...  -2.5717 -2.4617 -2.4884\n",
       "-2.6496 -2.5825 -2.5236  ...  -2.5747 -2.4598 -2.4925\n",
       "-2.6513 -2.5819 -2.5238  ...  -2.5748 -2.4597 -2.4939\n",
       "[torch.FloatTensor of size 499x13]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 버킷이랑 같이 쓰는 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BUCKETRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,bucket_config,hidden_size, num_layers, num_classes,vocab_size,embedding_dim):\n",
    "        self.models={}\n",
    "        self.optims={}\n",
    "        self.bucket_config=bucket_config\n",
    "        for i in range(len(self.bucket_config)):\n",
    "            self.models[i] = RNN(hidden_size, num_layers, num_classes,vocab_size,embedding_dim)\n",
    "            self.optims[i] = optim.Adam(self.models[i].parameters(), lr=0.001)\n",
    "            \n",
    "        \n",
    "    def select_bucket(self):\n",
    "        bucket_id = random.choice(range(len(bucket_config)))\n",
    "        \n",
    "        return bucket_id\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket_model = BUCKETRNN(bucket_config,HIDDEN_DIM, NUM_LAYERS,len(tag_to_ix),len(word_to_ix),EMBEDDING_DIM)\n",
    "loss_function =  nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss : 2.5338616371154785 , bucket : 0\n",
      "[100] loss : 1.021700382232666 , bucket : 3\n",
      "[200] loss : 0.46864187717437744 , bucket : 0\n",
      "[300] loss : 0.5949462652206421 , bucket : 1\n",
      "[400] loss : 0.9982641339302063 , bucket : 2\n",
      "[500] loss : 0.8244224786758423 , bucket : 2\n",
      "[600] loss : 0.6691949367523193 , bucket : 3\n",
      "[700] loss : 0.5334180593490601 , bucket : 1\n",
      "[800] loss : 0.3589295446872711 , bucket : 0\n",
      "[900] loss : 0.6886817216873169 , bucket : 3\n",
      "[1000] loss : 0.41534432768821716 , bucket : 2\n",
      "[1100] loss : 0.49127447605133057 , bucket : 3\n",
      "[1200] loss : 0.3872307240962982 , bucket : 2\n",
      "[1300] loss : 0.47533732652664185 , bucket : 2\n",
      "[1400] loss : 0.4393002986907959 , bucket : 3\n",
      "[1500] loss : 0.3600185215473175 , bucket : 3\n",
      "[1600] loss : 0.4524793326854706 , bucket : 2\n",
      "[1700] loss : 0.09196716547012329 , bucket : 0\n",
      "[1800] loss : 0.13422846794128418 , bucket : 0\n",
      "[1900] loss : 0.3615540564060211 , bucket : 2\n",
      "[2000] loss : 0.13525305688381195 , bucket : 0\n",
      "[2100] loss : 0.3106137812137604 , bucket : 1\n",
      "[2200] loss : 0.23308174312114716 , bucket : 3\n",
      "[2300] loss : 0.07280982285737991 , bucket : 0\n",
      "[2400] loss : 0.25790470838546753 , bucket : 2\n",
      "[2500] loss : 0.3075273633003235 , bucket : 2\n",
      "[2600] loss : 0.20128652453422546 , bucket : 1\n",
      "[2700] loss : 0.267413854598999 , bucket : 2\n",
      "[2800] loss : 0.2660099267959595 , bucket : 2\n",
      "[2900] loss : 0.2145916223526001 , bucket : 0\n",
      "[3000] loss : 0.18937240540981293 , bucket : 1\n",
      "[3100] loss : 0.13038747012615204 , bucket : 0\n",
      "[3200] loss : 0.26689308881759644 , bucket : 1\n",
      "[3300] loss : 0.16859322786331177 , bucket : 2\n",
      "[3400] loss : 0.08819016814231873 , bucket : 3\n",
      "[3500] loss : 0.18127848207950592 , bucket : 1\n",
      "[3600] loss : 0.1321011483669281 , bucket : 1\n",
      "[3700] loss : 0.14424817264080048 , bucket : 1\n",
      "[3800] loss : 0.12812256813049316 , bucket : 2\n",
      "[3900] loss : 0.12484750151634216 , bucket : 1\n",
      "[4000] loss : 0.12390623986721039 , bucket : 1\n",
      "[4100] loss : 0.09196265041828156 , bucket : 1\n",
      "[4200] loss : 0.12934979796409607 , bucket : 1\n",
      "[4300] loss : 0.10172194987535477 , bucket : 1\n",
      "[4400] loss : 0.0912589579820633 , bucket : 2\n",
      "[4500] loss : 0.061736565083265305 , bucket : 1\n",
      "[4600] loss : 0.08378574997186661 , bucket : 1\n",
      "[4700] loss : 0.05071571096777916 , bucket : 3\n",
      "[4800] loss : 0.11878789961338043 , bucket : 1\n",
      "[4900] loss : 0.02820456586778164 , bucket : 3\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5000):\n",
    "    \n",
    "    bucket_id = bucket_model.select_bucket()\n",
    "    inputs, targets,lengths = getBatch(bucket,bucket_id,BATCH_SIZE)\n",
    "    \n",
    "    bucket_model.models[bucket_id].zero_grad()\n",
    "    \n",
    "    outputs = bucket_model.models[bucket_id](inputs,lengths)\n",
    "    \n",
    "    loss = loss_function(outputs,targets)\n",
    "    losses.append(loss)\n",
    "    loss.backward()\n",
    "    bucket_model.optims[bucket_id].step()\n",
    "    \n",
    "    if epoch % 100==0:\n",
    "        print(\"[{epoch}] loss : {loss} , bucket : {bucket_id}\".format(epoch=epoch,loss=loss.data.numpy()[0],bucket_id=bucket_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "혹시 강동구 보건소 도 한 번 물어봐 주 세요 <PAD>\n",
      "\n",
      "O  :  O\n",
      "B-LOC  :  B-LOC\n",
      "I-LOC  :  I-LOC\n",
      "O  :  O\n",
      "O  :  O\n",
      "O  :  O\n",
      "O  :  O\n",
      "O  :  O\n",
      "O  :  O\n",
      "O  :  O\n"
     ]
    }
   ],
   "source": [
    "test = random.choice(training_data)\n",
    "input_ = test[0]\n",
    "tag = test[1]\n",
    "print(' '.join(input_)+'\\n')\n",
    "\n",
    "length = len(input_)\n",
    "for i in range(len(bucket_config)):\n",
    "        if bucket_config[i][0] == length:\n",
    "            bucket_id = i\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "sentence_in = prepare_sequence(input_,word_to_ix)\n",
    "sentence_in=sentence_in.view(1,-1)\n",
    "\n",
    "scores = bucket_model.models[bucket_id](sentence_in,[len(input_)])\n",
    "v,i = torch.max(scores,1)\n",
    "for t in range(i.size()[0]):\n",
    "    print(tag[t], ' : ', ix_to_tag[i.data.numpy()[t][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsksd/.local/lib/python3.5/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type BUCKETRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/dsksd/.local/lib/python3.5/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(bucket_model,'NER_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restore = torch.load('NER_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영화 예매 좀 해 줘\n",
      "\n",
      "O  :  O\n",
      "O  :  O\n",
      "O  :  O\n",
      "O  :  O\n",
      "O  :  O\n"
     ]
    }
   ],
   "source": [
    "test = random.choice(training_data)\n",
    "input_ = test[0]\n",
    "tag = test[1]\n",
    "print(' '.join(input_)+'\\n')\n",
    "\n",
    "length = len(input_)\n",
    "for i in range(len(bucket_config)):\n",
    "        if bucket_config[i][0] == length:\n",
    "            bucket_id = i\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "sentence_in = prepare_sequence(input_,word_to_ix)\n",
    "sentence_in=sentence_in.view(1,-1)\n",
    "\n",
    "scores = restore.models[bucket_id](sentence_in,[len(input_)])\n",
    "v,i = torch.max(scores,1)\n",
    "for t in range(i.size()[0]):\n",
    "    print(tag[t], ' : ', ix_to_tag[i.data.numpy()[t][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
