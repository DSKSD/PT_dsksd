{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd62590b798>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab;tagger=Mecab()\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 최대 길이 (10,10)으로 고정하고 PAD & Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH=10\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('../../dataset/corpus/dsksd_chat.txt').readlines()\n",
    "data = [[t.split('\\\\t')[0],t.split('\\\\t')[1][:-1]] for t in data if t !='\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_SIZE = len(data) # 배치 사이즈\n",
    "DATA_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 형태소 분석\n",
    "2. 최대 길이 10보다 긴 것들 10으로 제한\n",
    "3. EOS 태그 달기\n",
    "4. 길이 10이 안되는 것들 PADDING\n",
    "5. [[Q,A]...] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t0,t1 in data:\n",
    "    token0 = tagger.morphs(t0)\n",
    "    \n",
    "    if len(token0)>=SEQ_LENGTH:\n",
    "        token0= token0[:SEQ_LENGTH-1]\n",
    "    token0.append(\"EOS\")\n",
    "\n",
    "    token1 = tagger.morphs(t1)\n",
    "    if len(token1)>=SEQ_LENGTH:\n",
    "        token1=token1[:SEQ_LENGTH-1]\n",
    "    \n",
    "    token1.append(\"EOS\")\n",
    "    while len(token0)<SEQ_LENGTH:\n",
    "        token0.append('PAD')\n",
    "    \n",
    "    while len(token1)<SEQ_LENGTH:\n",
    "        token1.append('PAD')\n",
    "    \n",
    "    train.append([token0,token1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['끝말잇기', '고', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'],\n",
       " ['저', '바보', '라', '몰라요', 'ㅠㅠ', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인덱스 딕셔너리 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words=3\n",
    "word2index={\"SOS\":0,\"EOS\":1,\"PAD\":2}\n",
    "\n",
    "for t0,t1 in train:\n",
    "    for token in t0+t1:\n",
    "        if token not in word2index:\n",
    "            word2index[token]=n_words\n",
    "            n_words+=1\n",
    "\n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data to Tensor(LongTensor) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 토큰을 인덱스로 바꾼 후, LongTensor로 만든 후, autograd.Variable로 wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w: to_ix[w], seq))\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x=[]\n",
    "train_y=[]\n",
    "lengths=[]\n",
    "for tr in train:\n",
    "    temp = prepare_sequence(tr[0], word2index)\n",
    "    temp = temp.view(1,-1)\n",
    "    train_x.append(temp)\n",
    "\n",
    "    temp2 = prepare_sequence(tr[1],word2index)\n",
    "    temp2 = temp2.view(1,-1)\n",
    "    train_y.append(temp2)\n",
    "    \n",
    "    length = [t for t in tr[1] if t !='PAD']\n",
    "    lengths.append(len(length))\n",
    "\n",
    "inputs = torch.cat(train_x)\n",
    "targets = torch.cat(train_y)\n",
    "\n",
    "del train_x\n",
    "del train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '하', '세요', '!', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,batch_first=True)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, input.size(0), self.hidden_size)) \n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN (\n",
      "  (embedding): Embedding(455, 30)\n",
      "  (gru): GRU(30, 30, num_layers=2, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder_test = EncoderRNN(len(word2index), 30, 2)\n",
    "print(encoder_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([153, 10, 30]) torch.Size([2, 153, 30])\n"
     ]
    }
   ],
   "source": [
    "out, hidden = encoder_test(inputs.view(DATA_SIZE,-1))\n",
    "print(out.size(), hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder with Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n",
    "* https://arxiv.org/pdf/1409.0473.pdf\n",
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture10.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,max_len=10,n_layers=1,dropout_p=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_len=max_len\n",
    "\n",
    "        self.attn = nn.Linear(self.hidden_size,self.hidden_size) # Attention\n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "\n",
    "        #self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        self.gru = nn.GRU(self.hidden_size*2, self.hidden_size, self.n_layers,batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "    \n",
    "    def Attention(self, hidden, encoder_outputs):\n",
    "        \n",
    "        hidden = hidden.squeeze(0).unsqueeze(2)  # 히든 : (1,배치,차원) -> (배치,차원,1)\n",
    "        \n",
    "        batch_size = encoder_outputs.size(0) # B\n",
    "        max_len = encoder_outputs.size(1) # T\n",
    "        energies = self.attn(encoder_outputs.contiguous().view(batch_size*max_len,-1)) # B*T,D -> B*T,D\n",
    "        energies = energies.view(batch_size,max_len,-1) # B,T,D (배치,타임,차원)\n",
    "        \n",
    "        attn_energies = energies.bmm(hidden).transpose(1,2) # B,T,D * B,D,1 --> B,1,T \n",
    "        \n",
    "        alpha = F.softmax(attn_energies.squeeze(1)) # B,T\n",
    "        alpha = alpha.unsqueeze(1) # B,1,T\n",
    "        context = alpha.bmm(encoder_outputs) # B,1,T * B,T,D => B,1,D\n",
    "        \n",
    "        \n",
    "        return context # B,1,D\n",
    "    \n",
    "    \n",
    "    def forward(self, input,context,encoder_outputs,training=True):\n",
    "        \n",
    "        # Get the embedding of the current input word\n",
    "        embedded = self.embedding(input)\n",
    "        hidden = Variable(torch.zeros(self.n_layers, input.size(0), self.hidden_size)) \n",
    "        #embedded = self.dropout(embedded)\n",
    "        \n",
    "        decode=[]\n",
    "\n",
    "        # Apply GRU to the output so far\n",
    "        for i in range(self.max_len):\n",
    "\n",
    "\n",
    "            _, hidden = self.gru(torch.cat((embedded,context),2), hidden)\n",
    "            concated = torch.cat((hidden,context.transpose(0,1)),2)\n",
    "            score = self.out(concated.squeeze(0))\n",
    "            softmaxed = F.log_softmax(score)\n",
    "            decode.append(softmaxed)\n",
    "            _,input = torch.max(softmaxed,1)\n",
    "            embedded = self.embedding(input)\n",
    "            \n",
    "            # 그 다음 Context Vector를 Attention으로 계산\n",
    "            context = self.Attention(hidden, encoder_outputs) \n",
    "         \n",
    "        # if training:\n",
    "        # TODO 패딩이 아닌 진짜 length만 cost 계산하기...\n",
    "\n",
    "        # 요고 주의! time-step을 column-wise concat한 후, reshape!!\n",
    "        scores = torch.cat(decode,1)\n",
    "        return scores.view(input.size(0)*self.max_len,-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트레이닝 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 30\n",
    "LEARNING_RATE=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder =  EncoderRNN(len(word2index), HIDDEN_SIZE, 2)\n",
    "decoder = DecoderRNN(HIDDEN_SIZE,len(word2index))\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "enc_optim= optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "dec_optim = optim.Adam(decoder.parameters(),lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Variable(torch.LongTensor([[SOS_token]*DATA_SIZE])).transpose(1,0)\n",
    "outputs,context = encoder(inputs)\n",
    "\n",
    "score = decoder(decoder_input,context[-1].view(DATA_SIZE,1,-1),outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss : 6.2091264724731445\n",
      "[100] loss : 1.30751633644104\n",
      "[200] loss : 0.2609458267688751\n",
      "[300] loss : 0.07936285436153412\n",
      "[400] loss : 0.03935229405760765\n",
      "CPU times: user 4min 58s, sys: 4min 40s, total: 9min 38s\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "losses=[]\n",
    "for epoch in range(500):\n",
    "    \n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]*DATA_SIZE])).transpose(1,0)\n",
    "    outputs,context = encoder(inputs)\n",
    "\n",
    "    score = decoder(decoder_input,context[-1].view(DATA_SIZE,1,-1),outputs)\n",
    "    loss=loss_function(score,targets.view(-1))\n",
    "    losses.append(loss)\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), 5.0)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 5.0)\n",
    "    \n",
    "    enc_optim.step()\n",
    "    dec_optim.step()\n",
    "    \n",
    "    if epoch % 100==0:\n",
    "        print(\"[{epoch}] loss : {loss}\".format(epoch=epoch,loss=loss.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  너 성별 뭐 야\n",
      "\n",
      "A:  남자 요 !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = random.choice(range(DATA_SIZE))\n",
    "input_ = train[index][0]\n",
    "target = train[index][1]\n",
    "print('Q: ', ' '.join([i for i in input_ if i !='PAD' and i != 'EOS'])+'\\n')\n",
    "\n",
    "\n",
    "decoder_input = Variable(torch.LongTensor([[SOS_token]])).transpose(1,0)\n",
    "outputs,context = encoder(inputs[index].view(1,-1))\n",
    "\n",
    "score = decoder(decoder_input,context[-1].view(1,1,-1),outputs)\n",
    "\n",
    "v,i = torch.max(score,1)\n",
    "\n",
    "decoded=[]\n",
    "for t in range(i.size()[0]):\n",
    "    decoded.append(index2word[i.data.numpy()[t][0]])\n",
    "\n",
    "print('A: ', ' '.join([i for i in decoded if i !='PAD' and i != 'EOS'])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
