{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f63dd526630>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Sequence Models and LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "시퀀스 모델은 NLP의 중심이다. 이러한 모델들은 인풋들 간의 시간(step) 의존성을 가진다. 이러한 시퀀스 모델의 고전적인 예로는 POS 태깅을 위한 Hidden Markov Model, 혹은 Conditional Random Field이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LSTM's in Pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "예제를 시작하기 전, 몇 가지를 상기하자. Pytorch의 LSTM은 3D 텐서의 인풋을 기대한다. 텐서의 각 축의 의미는 중요하다.<br>\n",
    "1. 첫 번째 차원은 시퀀스 time-step\n",
    "2. 두 번째 차원은 미니-배치에서의 인덱스\n",
    "3. 세 번째 차원은 토큰 인덱스 in vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "일단 우리는 미니배치를 고려하지 않기 때문에, 2번째 차원은 항상 1로 간주하자. 만약 우리가 \"The cow jumped\"라는 문장을 사용하고 싶다면 우리의 인풋은 이렇게 생겼을 것이다.  $$ \n",
    "\\begin{bmatrix}\n",
    "\\overbrace{q_\\text{The}}^\\text{row vector} \\\\\n",
    "q_\\text{cow} \\\\\n",
    "q_\\text{jumped}\n",
    "\\end{bmatrix}\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd.Variable(torch.randn(1,1,3)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputs = [ autograd.Variable(torch.randn((1,3))) for _ in range(5) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Variable containing:\n",
       "  0.9610  0.3508 -0.1519\n",
       " [torch.FloatTensor of size 1x3], Variable containing:\n",
       "  0.5372 -1.2869  1.6373\n",
       " [torch.FloatTensor of size 1x3], Variable containing:\n",
       "  1.4175 -0.4246 -0.6304\n",
       " [torch.FloatTensor of size 1x3], Variable containing:\n",
       "  0.0919 -0.2338  1.3037\n",
       " [torch.FloatTensor of size 1x3], Variable containing:\n",
       " -1.0965 -0.0207 -1.4612\n",
       " [torch.FloatTensor of size 1x3]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].view(1,1,-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.6223 -0.0418 -0.1876\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "  1.1719 -0.3549 -0.3968\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3) # Input dim is 3, output dim is 3\n",
    "inputs = [ autograd.Variable(torch.randn((1,3))) for _ in range(5) ] # make a sequence of length 5 (5,1,3) 의 텐서??\n",
    "\n",
    "# 히든 스테이트 초기화 뭐지?!! \n",
    "# (h0, c0) hidden state, cell state \n",
    "hidden = (autograd.Variable(torch.randn(1,1,3)), autograd.Variable(torch.randn((1,1,3))))\n",
    "for i in inputs:\n",
    "    # 요렇게 스텝바이스텝 넘겨주는 방법도 있음\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1,1,-1), hidden)\n",
    "\n",
    "# 다른 방법으로, 전체 시퀀스를 한방에 넘길 수도 있다\n",
    "# out은 시퀀스 동안의 모든 hidden states이다.\n",
    "# hidden은 가장 최근 즉, out[-1]과 같음\n",
    "# out을 통해 모든 히든스테이트에 접근할 수 있고\n",
    "# hidden을 이용해서 추후, backprop을 할 수 있을 것.\n",
    "\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1) # Add the extra 2nd dimension (5,3) -> (5,1,3)\n",
    "hidden = (autograd.Variable(torch.randn(1,1,3)), autograd.Variable(torch.randn((1,1,3)))) # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out.size()[0])\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example: An LSTM for POS tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "이번에 우리는 POS 태깅을 위해 LSTM을 사용할 것이다. Viterbi나 Forward-Backward 등 그 어떤 것도 사용하지 않을 것이다. 하지만 독자들은 추후, Viterbi를 어떻게 사용할 수 있을지 생각해보아라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model is as follows: let our input sentence be $w_1, \\dots, w_M$, where $w_i \\in V$, our vocab. Also, let $T$ be our tag set, and $y_i$ the tag of word $w_i$. Denote our prediction of the tag of word $w_i$ by $\\hat{y}_i$.\n",
    "This is a structure prediction, model, where our output is a sequence $\\hat{y}_1, \\dots, \\hat{y}_M$, where $\\hat{y}_i \\in T$.\n",
    "To do the prediction, pass an LSTM over the sentence. Denote the hidden state at timestep $i$ as $h_i$. Also, assign each tag a unique index (like how we had word_to_ix in the word embeddings section). Then our prediction rule for $\\hat{y}_i$ is $$ \\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(Ah_i + b))_j $$ That is, take the log softmax of the affine map of the hidden state, and the predicted tag is the tag that has the maximum value in this vector. Note this implies immediately that the dimensionality of the target space of $A$ is $|T|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. 데이터 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w: to_ix[w], seq))\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 3, 'dog': 1, 'ate': 2, 'apple': 4, 'that': 7, 'read': 6, 'The': 0, 'book': 8, 'Everybody': 5}\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        # 만약 배치로 할거면, batch_first = True  옵션 줄 것.\n",
    "        # bidirectional = True 로 두면 bi-RNN \n",
    "        # num_layers 로 쌓을 수도\n",
    "        \n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # 초기 state 필요!\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)), # 히든 스테이트\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))  # 셀 스테이트 \n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence) # 5x6\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden) # 5x1x6\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1)) # 5x6 => 5x3\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. 트레이닝 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The', 'dog', 'ate', 'the', 'apple'], ['DET', 'NN', 'V', 'DET', 'NN'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       "[torch.LongTensor of size 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_sequence(train_data[0][0],word_to_ix)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(100): # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.  We need to clear them out\n",
    "        # before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 히든 스테이트 초기화\n",
    "        model.hidden = model.init_hidden()\n",
    "    \n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into Variables\n",
    "        # of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "    \n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "    \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by calling\n",
    "        # optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise: Augmenting the LSTM pos tagger with character-level features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the example above, each word had an embedding, which served as the inputs to our sequence model. Let's augment the word embeddings with a representation derived from the characters of the word. We expect that this should help significantly, since character-level information like affixes have a large bearing on part-of-speech. For example, words with the affix -ly are almost always tagged as adverbs in English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Do do this, let $c_w$ be the character-level representation of word $w$. Let $x_w$ be the word embedding as before. Then the input to our sequence model is the concatenation of $x_w$ and $c_w$. So if $x_w$ has dimension 5, and $c_w$ dimension 3, then our LSTM should accept an input of dimension 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. LSTM 모델이 2개 필요함. 하나는 POS tag scores를 계산하는 모델, 다른 하나는 각 단어의 character-level representation을 산출하는 모델\n",
    "2. character 모델을 실행하기 위해, characters를 임베딩해야 한다. character embeddings은 character LSTM의 인풋으로 들어갈 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "nltk의 brown 코퍼스에서  NUM_OF_DATA만큼의 문장을 가져온다. [[단어,단어,....],[POS,POS,...]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = nltk.corpus.brown.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NUM_OF_DATA=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = data[:NUM_OF_DATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "words=[]\n",
    "tags=[]\n",
    "vocab = []\n",
    "pos=[]\n",
    "i = 0\n",
    "for w,p in data:\n",
    "    i+=1\n",
    "    words.append(w)\n",
    "    tags.append(p)\n",
    "    \n",
    "    if w not in vocab:\n",
    "        vocab.append(w)\n",
    "    \n",
    "    if p not in pos:\n",
    "        pos.append(p)\n",
    "    \n",
    "    if p == '.' or i==NUM_OF_DATA:\n",
    "        train_data.append((words,tags))\n",
    "        words=[]\n",
    "        tags=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_to_ix = {v:i for i,v in enumerate(vocab)}\n",
    "tag_to_ix = {v:i for i,v in enumerate(pos)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "캐릭터 레벨의 dictionary도 준비한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "char_vocab = list(\"\"\"$%'`()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\\\\\n",
    "^_abcdefghijklmnopqrstuvwxyz{|}\"\"\")\n",
    "char_to_ix = {v:i for i,v in enumerate(char_vocab)}\n",
    "ix_to_char = {v:k for k,v in char_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def word2input(word):\n",
    "    chars = list(word)\n",
    "    chars_vec = list(map(lambda x:char_to_ix[x],chars))\n",
    "    return autograd.Variable(torch.LongTensor(chars_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. 모델링 : CHARLSTM  in LSTMTagger "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "각 단어를 character representation하여 CHAR_EMBEDDING 차원으로 만들고, 이를 기존의 word representation인 EMBEDDING_DIM 차원과 concat해서 Prediction하고 이를 기반으로 최적화한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CHAR_EMBEDDING = 10\n",
    "CHAR_HIDDEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CHARLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,char_size,embedding_dim,hidden_dim):\n",
    "        super(CHARLSTM,self).__init__()\n",
    "        self.embedding = nn.Embedding(char_size,embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim,self.hidden_dim)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)), # 히든 스테이트\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim))) \n",
    "    \n",
    "    def forward(self,word):\n",
    "        embeds = self.embedding(word)\n",
    "        output, self.hidden = self.lstm(embeds.view(len(word),1,-1), self.hidden)\n",
    "        \n",
    "        return self.hidden[0].view(1,self.hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 72\n",
       " 54\n",
       " 62\n",
       " 57\n",
       "[torch.LongTensor of size 4]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = random.choice(train_data)[0]\n",
    "test = random.choice(test)\n",
    "input_ = word2input(test)\n",
    "print(test)\n",
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 15\n",
    "HIDDEN_DIM = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size,char_size,char_embed_dim,char_hidden_dim):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.char_model = CHARLSTM(char_size,char_embed_dim,char_hidden_dim)\n",
    "        self.char_model.init_hidden()\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        # 만약 배치로 할거면, batch_first = True  옵션 줄 것.\n",
    "        # bidirectional = True 로 두면 bi-RNN \n",
    "        # num_layers 로 쌓을 수도\n",
    "        \n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim+char_hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # 초기 state 필요!\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)), # 히든 스테이트\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))  # 셀 스테이트 \n",
    "        \n",
    "    def forward(self, sentence,chars_embed):\n",
    "        embeds = self.word_embeddings(sentence) # n x 15\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden) # n x 1 x 15\n",
    "        lstm_out = lstm_out.view(len(sentence), -1)\n",
    "        \n",
    "        lstm_cat_out=[]\n",
    "        \n",
    "        \n",
    "        # 각 단어의 캐릭터 레벨의 representation을 가져와서 concat하고 최종 score를 계산한다!!\n",
    "        for i in range(len(sentence)):\n",
    "            self.char_model.init_hidden()\n",
    "            lstm_cat_out.append(torch.cat([lstm_out[i].view(1,self.hidden_dim),self.char_model(chars_embed[i])],1))\n",
    "        input_ = torch.cat(lstm_cat_out)\n",
    "        tag_space = self.hidden2tag(input_) # \n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. 트레이닝, 시간이 넘 오래걸림.. 잘못짠거 아님?ㅠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix),len(char_vocab),CHAR_EMBEDDING,CHAR_HIDDEN)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "losses=[]\n",
    "for epoch in range(10): # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    \n",
    "    if epoch%10==0: \n",
    "        \n",
    "        print(epoch)\n",
    "    \n",
    "    for sentence, tags in train_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.  We need to clear them out\n",
    "        # before each instance\n",
    "        model.zero_grad()\n",
    "        # 히든 스테이트 초기화\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        sentence_in = prepare_sequence(sentence,word_to_ix)\n",
    "        words_in = [word2input(w) for w in sentence]\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "    \n",
    "        # Step 3. Run our forward pass.\n",
    "       \n",
    "        tag_scores = model(sentence_in,words_in)\n",
    "    \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by calling\n",
    "        # optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        losses.append(loss)\n",
    "        loss.backward(retain_variables=True)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.5504\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.3544\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(losses[0],losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Variables.backward(retrain_variables=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "retain_variables (bool) – If True, buffers necessary for computing gradients won’t be freed after use. It is only necessary to specify True if you want to differentiate some subgraph multiple times (in some cases it will be much more efficient to use autograd.backward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "요약하자면 모델 안에 서브모델을 여러번 미분하고싶을 때 True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "지금 여기에도 LSTM tagger 안에 CharLSTM이 있기 땜에 일케 하는게 맞는거 같은데 과연 ..?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. 여튼 테스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ix_to_tag = {v:k for k,v in tag_to_ix.items()}  # 인덱스를 다시 태그로 돌리는 딕"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 디코딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced ``\n",
      "\n",
      "DET  :  DET\n",
      "NOUN  :  NOUN\n",
      "NOUN  :  NOUN\n",
      "ADJ  :  ADP\n",
      "NOUN  :  NOUN\n",
      "VERB  :  VERB\n",
      "NOUN  :  NOUN\n",
      "DET  :  NOUN\n",
      "NOUN  :  NOUN\n",
      "ADP  :  ADP\n",
      "NOUN  :  NOUN\n",
      "ADJ  :  NOUN\n",
      "NOUN  :  NOUN\n",
      "NOUN  :  NOUN\n",
      "VERB  :  VERB\n",
      ".  :  .\n"
     ]
    }
   ],
   "source": [
    "test = random.choice(train_data)\n",
    "print(' '.join(test[0])+'\\n')\n",
    "\n",
    "sentence_in = prepare_sequence(test[0],word_to_ix)\n",
    "words_in = [word2input(w) for w in test[0]]\n",
    "tag_scores = model(sentence_in,words_in)\n",
    "v,i = torch.max(tag_scores,1)\n",
    "for t in range(i.size()[0]):\n",
    "    print(test[1][t], ' : ', ix_to_tag[i.data.numpy()[t][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "오... 되긴하는구먼! 과적합인가? 음음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
