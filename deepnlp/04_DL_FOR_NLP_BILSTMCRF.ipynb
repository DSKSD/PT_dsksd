{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc4ec05f5e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### LSTM이 제공하는 features를 사용하는 CRF 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "모든 가능한 state의 집합을 S, 인풋 시퀀스를 x라고 할 때, CRF는 아래와 같은 모델을 만드는 것\n",
    "<br>\n",
    "$$p(s_1 . . . s_m|x_1 . . . x_m) = p(s|x)$$<br>\n",
    "CRF의 중요 아이디어 중 하나는 feature function을 정의하는 것이다. <br>\n",
    "$$Φ(x, s) ∈ R^d$$ <br>\n",
    "feature function은 전체 인풋 시퀀스 x를 (state 시퀀스 s, d차원 feature vector)로 매핑한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Write the recurrence for the viterbi variable at step i for tag k.\n",
    "* Modify the above recurrence to compute the forward variables instead.\n",
    "* Modify again the above recurrence to compute the forward variables in log-space (hint: log-sum-exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$$ P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "y가 태그 시퀀스, x가 단어들의 인풋 시퀀스라고 했을 때, 위 식을 계산한다<br>\n",
    "Where the score is determined by defining some log potentials $\\log \\psi_i(x,y)$ such that $$ \\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### emission and trainstion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "emission potential for the word at index i는 Bi-LSTM의 timestep i번째 히든 스테이트로부터 온다. transition score는 T x T 크기의 매트릭스 P에 저장된다. (T는 tagset) <br><br>\n",
    "$$ \\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i) $$$$ = \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 헬퍼 펑션 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Helper functions to make the code more readable.\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w: to_ix[w], seq))\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "def to_scalar(var):\n",
    "    # returns a python float\n",
    "    return var.view(-1).data.tolist()[0]\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return to_scalar(idx)\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "# Forward 알고리즘을 수치적으로 안정시키기 위해 곱셈을 log 취해서 Sum 한다.\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 모델링 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The partition function is an integral (for continuous variables) or sum (for discretevariables) over the unnormalized probability of all states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$$ \\alpha_i(j) = \\sum_{j' \\in T} \\psi_\\text{EMIT}(j \\rightarrow i) \\times \\psi_\\text{TRANS}(j' \\rightarrow j) \\times \\alpha_{i-1}(j') $$\n",
    "$$ \\log \\alpha_i(j) = \\log \\overbrace{\\sum_{j' \\in T} \\exp{(\\log \\psi_\\text{EMIT}(j \\rightarrow i) + \\log \\psi_\\text{TRANS}(j' \\rightarrow j) + \\log \\alpha_{i-1}(j'))}}^\\text{transform out of log-space and compute forward variable} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### function ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-10000 -10000 -10000 -10000 -10000\n",
       "[torch.FloatTensor of size 1x5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(1, 5).fill_(-10000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n",
      "\n",
      " 1  1  1  1\n",
      " 2  2  2  2\n",
      " 3  3  3  3\n",
      "[torch.FloatTensor of size 3x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1], [2], [3]])\n",
    "print(x.size())\n",
    "print(x)\n",
    "print(x.expand(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        \n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, num_layers=1, bidirectional=True)\n",
    "        # forward 와 backward hidden vector를 concat하는듯?\n",
    "        \n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        \n",
    "        # 트렌지션 매트릭스 \n",
    "        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # bi-direc이니까 히든스테이트도 2개씩\n",
    "        return ( autograd.Variable( torch.randn(2, 1, self.hidden_dim)),\n",
    "                 autograd.Variable( torch.randn(2, 1, self.hidden_dim)) )\n",
    "    \n",
    "    \n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.) # 1xTagset_size\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0. # 시작 태그의 score는 0, 나머지는 -10000\n",
    "        \n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = autograd.Variable(init_alphas)\n",
    "        \n",
    "        # Iterate through the sentence\n",
    "        for feat in feats: #문장의 피처들\n",
    "            alphas_t = [] # The forward variables at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of the previous tag\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the edge (i -> next_tag)\n",
    "                # before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "        \n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "        \n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = autograd.Variable( torch.Tensor([0]) )\n",
    "        tags = torch.cat( [torch.LongTensor([self.tag_to_ix[START_TAG]]), tags] )\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + self.transitions[tags[i+1], tags[i]] + feat[tags[i+1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "    \n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        \n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "        \n",
    "        # forward_var at step i holds the viterbi variables for step i-1 \n",
    "        forward_var = autograd.Variable(init_vvars)\n",
    "        for feat in feats:\n",
    "            bptrs_t = [] # holds the backpointers for this step\n",
    "            viterbivars_t = [] # holds the viterbi variables for this step\n",
    "            \n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the previous step,\n",
    "                # plus the score of transitioning from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id])\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "        \n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "        \n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG] # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "        \n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        self.hidden = self.init_hidden()\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "        \n",
    "    def forward(self, sentence): # dont confuse this with _forward_alg above.\n",
    "        self.hidden = self.init_hidden()\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        \n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 데이터 준비 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 토이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "# Make up some training data\n",
    "training_data = [ (\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ") ]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "tag_to_ix = { \"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 리얼 with Batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = json.load(open('../../dataset/corpus/NER_16000_train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_data=[]\n",
    "\n",
    "for sent in train:\n",
    "    if len(sent)==0: continue\n",
    "    word=[]\n",
    "    tag=[]\n",
    "    for w,p,t in sent:\n",
    "        word.append(w)\n",
    "        tag.append(t)\n",
    "    training_data.append((word,tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NER_LIST = ['B-PER','I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG','B-DATE', 'I-DATE','B-TIME','I-TIME','B-MISC','I-MISC','O',\"<START>\",\"<STOP>\"]\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "ix_to_word = {v:k for k,v in word_to_ix.items()}\n",
    "\n",
    "tag_to_ix={}\n",
    "i=0\n",
    "for tag in NER_LIST:           \n",
    "    tag_to_ix[tag] = i\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      " 25.0961\n",
      "[torch.FloatTensor of size 1]\n",
      ", [7, 0, 5, 0, 5, 0, 5, 0, 5, 0, 4, 11])\n"
     ]
    }
   ],
   "source": [
    "# Check predictions before training\n",
    "precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "precheck_tags = torch.LongTensor([ tag_to_ix[t] for t in training_data[0][1] ])\n",
    "print(model(precheck_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi 10'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hi {}\".format(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] epoch LOSS : 0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "for epoch in range(300): # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    loss=0\n",
    "    if epoch % 10==0:\n",
    "        print('[{}] epoch LOSS : {}'.format(epoch,loss))\n",
    "        \n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.  We need to clear them out\n",
    "        # before each instance\n",
    "        model.zero_grad()\n",
    "    \n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into Variables\n",
    "        # of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.LongTensor([ tag_to_ix[t] for t in tags ])\n",
    "    \n",
    "        # Step 3. Run our forward pass.\n",
    "        neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\n",
    "        loss = neg_log_likelihood\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by calling\n",
    "        # optimizer.step()\n",
    "        neg_log_likelihood.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      " 36.6214\n",
      "[torch.FloatTensor of size 1]\n",
      ", [0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Check predictions after training\n",
    "precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "print(model(precheck_sent))\n",
    "# We got it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
