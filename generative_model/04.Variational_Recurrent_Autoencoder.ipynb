{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab;tagger=Mecab()\n",
    "from collections import Counter\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://arxiv.org/pdf/1412.6581.pdf\n",
    "* https://arxiv.org/pdf/1511.06349.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('../../dataset/corpus/DOMAIN_10D_300EA_DATA_170427.txt','r',encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [d.split('\\t')[0] for d in data if 'FLOWER' in d.split('\\t')[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH=15\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t0 in data:\n",
    "    t0 = t0.replace(\"<br>\",\"\")\n",
    "    t0 = t0.replace(\"/\",\"\")\n",
    "    \n",
    "    token0 = tagger.morphs(t0)\n",
    "    \n",
    "    if len(token0)>=SEQ_LENGTH:\n",
    "        token0= token0[:SEQ_LENGTH-1]\n",
    "    token0.append(\"EOS\")\n",
    "\n",
    "    while len(token0)<SEQ_LENGTH:\n",
    "        token0.append('PAD')\n",
    "    \n",
    "    train.append([token0,token0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words=4\n",
    "word2index={\"SOS\":0,\"EOS\":1,\"PAD\":2,\"UNK\":3}\n",
    "\n",
    "for t in train:\n",
    "    for token in t[0]:\n",
    "        if token not in word2index:\n",
    "            word2index[token]=n_words\n",
    "            n_words+=1\n",
    "\n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_list(x):\n",
    "    del x[:]\n",
    "    del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w: to_ix[w], seq))\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x=[]\n",
    "train_y=[]\n",
    "lengths=[]\n",
    "for tr in train:\n",
    "    temp = prepare_sequence(tr[0], word2index)\n",
    "    temp = temp.view(1,-1)\n",
    "    train_x.append(temp)\n",
    "\n",
    "    temp2 = prepare_sequence(tr[1],word2index)\n",
    "    temp2 = temp2.view(1,-1)\n",
    "    train_y.append(temp2)\n",
    "    \n",
    "    length = [t for t in tr[1] if t !='PAD']\n",
    "    lengths.append(len(length))\n",
    "\n",
    "inputs = torch.cat(train_x)\n",
    "targets = torch.cat(train_y)\n",
    "\n",
    "remove_list(train_x)\n",
    "remove_list(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1,latent_size=10):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.linear = nn.Linear(hidden_size,latent_size*2)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,batch_first=True)\n",
    "    \n",
    "    def reparametrize(self, mu, log_var):\n",
    "        \"\"\"\"z = mean + eps * sigma where eps is sampled from N(0, 1).\"\"\"\n",
    "        eps = Variable(torch.randn(mu.size(0), mu.size(1)))\n",
    "        z = mu + eps * torch.exp(log_var/2)    # 2 for convert var to std\n",
    "        return z\n",
    "    \n",
    "    def forward(self, input,train=True):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, input.size(0), self.hidden_size)) \n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        h = self.linear(hidden[-1])\n",
    "        mu, log_var = torch.chunk(h, 2, dim=1)  # mean and log variance.\n",
    "        z = self.reparametrize(mu, log_var)\n",
    "        \n",
    "        return z,mu,log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN (\n",
      "  (linear): Linear (100 -> 20)\n",
      "  (embedding): Embedding(780, 100)\n",
      "  (gru): GRU(100, 100, num_layers=2, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder_test = EncoderRNN(len(word2index), 100, 2)\n",
    "print(encoder_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, mu,log_var = encoder_test(inputs[:3].view(3,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1,latent_size=10):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.linear = nn.Linear(latent_size,hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        #self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        # 요 부분에서 y_{t-1}과 c를 concat해서 넣어준다..!\n",
    "        # 이게 논문에서 제시하는 방법과는 조금 다른듯..\n",
    "        # gru의 내부까지 컨트롤 할 방법이 없으니 (직접 짜지 않는 이상)\n",
    "        self.gru = nn.GRU(self.hidden_size*2, self.hidden_size, self.n_layers,batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "        \n",
    "    def forward(self, input,latent,lengths,seq_length,training=True):\n",
    "        \n",
    "        # Get the embedding of the current input word\n",
    "        embedded = self.embedding(input)\n",
    "        hidden = Variable(torch.zeros(self.n_layers, input.size(0), self.hidden_size))\n",
    "        if training:\n",
    "            context = self.tanh(self.linear(latent)).view(BATCH_SIZE,1,-1)\n",
    "        else:\n",
    "            context = self.tanh(self.linear(latent)).view(1,1,-1)\n",
    "        #embedded = self.dropout(embedded)\n",
    "        \n",
    "        decode=[]\n",
    "        # Apply GRU to the output so far\n",
    "        for i in range(seq_length):\n",
    "            \n",
    "            \n",
    "            _, hidden = self.gru(torch.cat((embedded,context),2), hidden)\n",
    "            concated = torch.cat((hidden,context.transpose(0,1)),2)\n",
    "            score = self.out(concated.squeeze(0))\n",
    "            softmaxed = F.log_softmax(score)\n",
    "            decode.append(softmaxed)\n",
    "            _,input = torch.max(softmaxed,1)\n",
    "            embedded = self.embedding(input)\n",
    "            #embedded = self.dropout(embedded)\n",
    "        \n",
    "        # if training:\n",
    "        # TODO 패딩이 아닌 진짜 length만 cost 계산하기...\n",
    "            \n",
    "        # 요고 주의! time-step을 column-wise concat한 후, reshape!!\n",
    "        scores = torch.cat(decode,1)\n",
    "        remove_list(decode)\n",
    "        \n",
    "        return scores.view(input.size(0)*seq_length,-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 30\n",
    "LEARNING_RATE=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder =  EncoderRNN(len(word2index), HIDDEN_SIZE, 2)\n",
    "decoder = DecoderRNN(HIDDEN_SIZE,len(word2index))\n",
    "\n",
    "Recon = nn.CrossEntropyLoss()\n",
    "KLD = nn.KLDivLoss()\n",
    "enc_optim= torch.optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "dec_optim = torch.optim.Adam(decoder.parameters(),lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100] ELBO : 4.6391143798828125 , RECON : 4.607559680938721 & KLD : 0.031554847955703735 <0.5>\n",
      "[200] ELBO : 4.410755634307861 , RECON : 4.401738166809082 & KLD : 0.009017646312713623 <0.5>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-ff9bc58cce60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mELBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mELBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dsksd/.local/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    \n",
    "    #KCA = 0.3\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    \n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]*BATCH_SIZE])).transpose(1,0)\n",
    "    normal = Variable(torch.randn([1,BATCH_SIZE,HIDDEN_SIZE])) \n",
    "    latent, mu, log_var = encoder(inputs)\n",
    "\n",
    "    score = decoder(decoder_input,latent,lengths,SEQ_LENGTH)\n",
    "    recon_loss=Recon(score,targets.view(-1))\n",
    "    kld_loss = torch.sum(0.5 * (mu**2 + torch.exp(log_var) - log_var -1))\n",
    "    #checker.append((recon_loss,kld_loss))\n",
    "    \n",
    "#     KL_COST_ANNEALING\n",
    "#     if recon_loss.data.numpy()[0]<1.0:\n",
    "#         KCA = 0.9\n",
    "\n",
    "#     else:\n",
    "#         KCA = 0.5\n",
    "    ELBO = recon_loss+kld_loss\n",
    "    loss = ELBO.data.numpy()[0]\n",
    "    \n",
    "    ELBO.backward()\n",
    "    \n",
    "    \n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), 5.0)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 5.0)\n",
    "    \n",
    "    enc_optim.step()\n",
    "    dec_optim.step()\n",
    "    epoch+=1\n",
    "    \n",
    "    if epoch % 100==0:\n",
    "        #kindex+=1\n",
    "        print(\"[{epoch}] ELBO : {ELBO} , RECON : {RECON} & KLD : {KLD} <{KCA}>\".format(epoch=epoch,ELBO=ELBO.data.numpy()[0],\n",
    "                                                                              RECON=recon_loss.data.numpy()[0],\n",
    "                                                                              KLD=kld_loss.data.numpy()[0],\n",
    "                                                                              KCA=KCA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  화환 배달 가능 합니까\n",
      "\n",
      "A:  화환 배달 가능 합니까\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index=random.choice(range(300))\n",
    "_,context = encoder(inputs[index].view(1,-1))\n",
    "decoder_input = Variable(torch.LongTensor([[SOS_token]])).transpose(1,0)\n",
    "#context = Variable(torch.randn([1,1,HIDDEN_SIZE])) \n",
    "recon = decoder(decoder_input,context[-1].view(1,1,-1),lengths,SEQ_LENGTH)\n",
    "\n",
    "v,i = torch.max(recon,1)\n",
    "\n",
    "decoded=[]\n",
    "for t in range(i.size()[0]):\n",
    "    decoded.append(index2word[i.data.numpy()[t][0]])\n",
    "    \n",
    "print('Q: ', ' '.join([i for i in train[index][0] if i !='PAD' and i != 'EOS'])+'\\n')\n",
    "print('A: ', ' '.join([i for i in decoded if i !='PAD' and i != 'EOS'])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.4746\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index=random.choice(range(300))\n",
    "_,context = encoder(inputs[index].view(1,-1))\n",
    "normal = Variable(torch.randn([1,1,HIDDEN_SIZE]))\n",
    "kld_loss = KLD(context[-1],normal)\n",
    "print(kld_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  꽃 꽃 화환 화환\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder_input = Variable(torch.LongTensor([[SOS_token]])).transpose(1,0)\n",
    "context = Variable(torch.randn([1,10])) \n",
    "recon = decoder(decoder_input,context,lengths,SEQ_LENGTH,False)\n",
    "\n",
    "v,i = torch.max(recon,1)\n",
    "\n",
    "decoded=[]\n",
    "for t in range(i.size()[0]):\n",
    "    decoded.append(index2word[i.data.numpy()[t][0]])\n",
    "\n",
    "print('A: ', ' '.join([i for i in decoded if i !='PAD' and i != 'EOS'])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
